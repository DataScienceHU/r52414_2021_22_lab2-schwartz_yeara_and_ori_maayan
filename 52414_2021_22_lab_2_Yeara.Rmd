---
title: "52414 - lab 2 "
author: "52414"
date: "25/5/2022"
output: html_document
---


# *Lab 2: Text analysis, Sampling and inference*  
<br/><br/>  
  
### Submission Instructions  

Your final push should include this Rmd file (with your answers filled-in), together with the html file that is outputted automatically by knitr when you knit the Rmd. Anything else will be disregarded. In addition, please adhere to the following file format:    
`Lab_2_FamilyName1_Name1_and_FamilyName2_Name2.Rmd/html`      

<br/><br/>

The only allowed libraries are the following (**please do not add your own without consulting the course staff**):
```{r, include=FALSE}
library(tidyverse) # Includes dplyr, stringr, ggplot2, .. 
library(data.table)
library(ggthemes)
library(stringr)
library(tidytext) 
library(rvest)
library(ggrepel)
```  
<br/><br/>

## Analysis of textual data and the `Wordle` game 
    
In this lab we will analyze textual data from the web. We will compute several statistics, and 
also implement and solve the popular game [wordle](https://en.wikipedia.org/wiki/Wordle).   

**Solution:**  
[INSERT YOUR TEXT, CODE, PLOTS AND TABLE HERE, SEPERATED INTO SUB-QUESTIONS]

# PART 1 - MOBY-DICK
## 1. 
### section a
```{r}
url <- "https://www.gutenberg.org/files/2701/2701-h/2701-h.htm"
moby_dick <- read_html(url)

txt_moby <- html_text(html_nodes(moby_dick, "body"))
# clean /r/n/r/n from the first 5 charateres.
txt_moby <- str_sub(txt_moby, 5, nchar(txt_moby))
# txt_moby <- gsub("\r\n","",txt_moby)
first_line <- str_sub(txt_moby, 1, nchar(gsub("(^[^\r\n]*).*$", "\\1", txt_moby)))
print(first_line)
```

### section b
```{r}
clean_moby <- unlist(strsplit(txt_moby, split = "[ ]|[.]|[\n]|[\r]|[,]"))
dat_moby <- data.frame("word" = clean_moby)
dat_moby <- filter(dat_moby, word!="")
dat_moby$length <- as.numeric(nchar(dat_moby$word))


paste0("There are ", length(dat_moby$word), " words")
ggplot(dat_moby, aes(length)) + labs(title="Words' Length Dist.", x = "Word Length") + geom_bar(fill = "#0073C2FF")

```
[There are words of length over 20 because at this part we aere asked to leave words that are seperated by - or ?.]


```{r}
words_length <- dat_moby %>% group_by(length) %>% summarize(freq = n()) %>% arrange(-freq)

paste0("Median: ", median(dat_moby$length))
paste0("Mean: ", mean(dat_moby$length))
paste0("Longest word: ", max(dat_moby$length), " characters.")
paste0("Most common word length: ", words_length$length[1], " characters.")
```


### section c
```{r}
moby_words_frequency <- dat_moby %>% group_by(word) %>% summarise(freq = n()) %>% arrange(-freq)

#10 most common words
head(moby_words_frequency$word, n=10)
```
[It is not surprising that 10 most common words are all determiners. These words are essential in writing and are always relevant regardless of text content.]

## 2.
### section a
```{r}
#find location of first chapter
loc_chap1 <- str_locate_all(txt_moby, regex("CHAPTER \\d. Loomings."))
#text of all chapters without first parts.
all_chap <- str_sub(txt_moby, 35376, nchar(txt_moby))

#split by chapters
loc_chaps <- str_locate_all(all_chap, regex("CHAPTER \\d+\\. \\w+"))
starters <- loc_chaps[[1]][,1]

split_to_chap <- function(loc_vec, my_text){
  txt_by_chap <- data.frame()
  for(i in (1:length(loc_vec)-1)){
    cur_content <- str_sub(my_text, loc_vec[i], (loc_vec[i+1]-1))
    txt_by_chap <- rbind(txt_by_chap, cur_content)
  }
  txt_by_chap <- rbind(txt_by_chap, str_sub(my_text, loc_vec[length(loc_vec)], nchar(my_text)))
  txt_by_chap$chapter_num <- 1:135
  colnames(txt_by_chap) <- c("chapter_content", "chapter_num")
  txt_by_chap <- relocate(txt_by_chap, "chapter_num")
  return(txt_by_chap)
}

dat_chap <- split_to_chap(starters, all_chap)


#words for each chapter
count_words <- function(txt){
  all_words <- unlist(strsplit(txt, split = "[ ]|[.]|[\n]|[\r]|[,]"))
  only_words <- data.frame("word" = all_words)
  clean_words <- filter(only_words, word!="")
  num_of_words <- as.numeric(length(clean_words$word))
  return(num_of_words)
}

dat_chap$num_of_words <- sapply(dat_chap$chapter_content, count_words)

#plot
ggplot(dat_chap, aes(x=chapter_num, y=num_of_words)) + labs(title="Number of Words by Chapter", x = "Chapter", y="Num of Words") + geom_bar(stat="identity", fill = "#0073C2FF", width = 0.5) + theme(axis.text.x = element_text(angle = 90, vjust =0.5, hjust=0.5))

```
[explain why we chose to use CHAPTER's titles in chapter's content.]


### section b
```{r}
find_words <- function(my_word, search_array){
  search_word <- as.character(my_word)
  result_vec <- c()
  for(i in search_array){
    word_count <- str_count(as.character(dat_chap$chap_words[i]), search_word)
    cur_relative_freq <- word_count/dat_chap$num_of_words[i]
    result_vec <- c(result_vec, cur_relative_freq)
  }
  return(result_vec)
}

#Moby search

#find Ahab
ggplot(dat_chap, aes(x=chapter_num, y=find_words("Ahab", 1:135))) + labs(title="Relative Frequency of 'Ahab' by Chapter", x = "Chapter", y="Relative frequency") + geom_bar(stat="identity", fill = "#0073C2FF", width = 0.5) + theme(axis.text.x = element_text(angle = 90, vjust =0.5, hjust=0.5))

#find Moby
ggplot(dat_chap, aes(x=chapter_num, y=find_words("Moby", 1:135))) + labs(title="Relative Frequency of 'Moby' by Chapter", x = "Chapter", y="Relative frequency") + geom_bar(stat="identity", fill = "#0073C2FF", width = 0.5) + theme(axis.text.x = element_text(angle = 90, vjust =0.5, hjust=0.5))

#find sea
ggplot(dat_chap, aes(x=chapter_num, y=find_words("sea", 1:135))) + labs(title="Relative Frequency of 'sea' by Chapter", x = "Chapter", y="Relative frequency") + geom_bar(stat="identity", fill = "#0073C2FF", width = 0.5) + theme(axis.text.x = element_text(angle = 90, vjust =0.5, hjust=0.5))
```
[Do you see a different behavior for the different words? in which parts of the book are they frequent?
The word Ahab is frequent for almost 50% of the book, split to two parts- starting Chapter 16 for the first, and Chapter 99 for the second.
The word Moby is frequent mostly at the second quarter of the book and at the last chapters.
The word sea is frequent for most of the chapters of books.]

## 3.
### section a
#### Answer 1
```{r}
clean_moby <- stringi::stri_remove_empty(clean_moby)
prob_same_word1 <- sum(moby_words_frequency$freq^2)/(length(clean_moby)^2)
```

#### Answer 2
```{r}
B = 100000
set.seed(2022)

stim_prob <- function(num){
  count = 0
  for(i in 1:num){
  A_choice <- sample(clean_moby, 1)
  B_choice <- sample(clean_moby, 1)
  count <- ifelse(A_choice==B_choice, count+1, count)
  }
  return(count/num)
}
prob_same_word2 <- stim_prob(B)
```

### section b
```{r}

options(scipen = 999)
num_unique_words <- length(unique(clean_moby))
prob_same_word3 <- 1/num_unique_words
```

# PART 2 - WORDLE

## 1. 
### section a
```{r}
word_bank <- read_file("https://www-cs-faculty.stanford.edu/~knuth/sgb-words.txt")
words_vec <- strsplit(word_bank,split = '\n')
loc <- (1:5)
letter <- letters
table_26on5 <- data.frame()

for(let in letter){
  for(i in loc){
    new_vec <- c(new_vec ,count(words_vec[][i]==let))
  }
  table_26on5 <- append(new_vec)
}
```


```{r, cache=TRUE}
# Helper function: 
wordle_match <- function(guess, word)  # 1: correct location, -1: wrong location, 0: missing
{
  L <- nchar(guess)
  match <- rep(0, L)
  for(i in 1:L)
  {
    if(grepl(substr(guess, i, i), word, fixed=TRUE))
      {match[i] = -1}
    if(substr(guess, i, i) == substr(word, i, i))
    {      match[i] = 1}
  }
  
  return(match)
}

```


